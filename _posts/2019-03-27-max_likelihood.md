---
title: 逻辑回归与极大似然估计
categories: ['机器学习']
tags: []
resource_path: /blog/assets/2019/
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

逻辑回归与极大似然估计
===

极大似然估计就是已知结果反推参数的过程。其根本思想是假设当前情况即为概率最大的情况。

在逻辑回归中，可以把每一个样本看作是一次独立随机试验,且每次实验的结果为0和1的概率各不相同。假设不包括重复的样本，即每个样本可由它的特征值组合来确定。任意一次实验的结果为1，即任一样本其标签为1的概率可由参数
$$\theta$$
来确定：  

$$
p(y=1|x) = sigmoid(\theta_0 + \theta_1x_1+\theta_2x_2+...+\theta_nx_n)
$$

由于每次实验只有1和0两个结果，我们也很容易得出该特征值组合出现且标签为0的概率：

$$
p(y=0|x) = 1-p(y=0|x)
$$

先假设训练集中样本的顺序是确定的，训练集为：
$$ \{ (x^{(1)}, 1),\ (x^{(2)},0),\ (x^{(3)},1) \} $$
，把
$$x^{(i)}$$
看作是第i次独立实验，我们很容易就可以得出这种组合情况出现的概率：

$$
p(D) = p(y=1|x^{(1)}) \cdot p(y=0|x^{(2)}) \cdot p(y=1|x^{(3)})
$$

令各个样本
$$x^{(i)}$$
对应的标签为
$$y^{(i)}$$
,推广到m个样本上：

$$
p(D) = p(y=y^{(1)}|x^{(1)}) \cdot p(y=y^{(2)}|x^{(2)})\cdot\ ...\ \cdot p(y=y^{(m)}|x^{(m)})
$$

我们可以认为当前结果即为该m次实验的实验结果中的最可能的一种，概率最大的一种，而且越大越好。我们要做的就是求上面这个式子所能达到的最大值
$$\max (p(D))$$
。

由于有序和无序的多次实验，在次数确定的情况下，其概率公式只相差一个排列组合的常系数，故在求最大值的过程中可以省略。

显然，上面的公式并不好求最大值，我们可以利用对数函数单调的性质，对式子进行处理：

$$
\begin{align}
P(D)&= ln(p(D))\\
&= \sum_{i=1}^m p(y=y^{(i)}|x^{(i)})\\
&= \sum_{i=1}^m y^{(i)}p(y=1|x^{(i)}) + (1-y^{(i)})(1-p(y=1|x^{(i)}))
\end{align}

$$