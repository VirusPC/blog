---
title: 深入正则化
categories: ['机器学习']
tags: []
resource_path: /blog/assets/2019/03/28/regularization
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

正则化
===

---

引出
---

给定数据集
$$ D=\{(x_1,y_1),\ (x_2,y_2),\ ...,\ (x_m,y_m) \} $$
，其中
$$ x \in \mathbb{R}^d $$
，
$$ y \in \mathbb{R} $$
，我们考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为：

$$ { \min_{w} \sum_{i=1}^m (y_i-w^Tx_i)^2 } $$

当特征很多而数据集不够大时，该模型会陷入过拟合。为了缓解过拟合问题，我们引入了正则化项（regularization item）。将目标函数与正则化项相加是为正则化。

范数
---

正则化有两种方式，一种是L1正则化，一种是L2正则化。其中，前者是利用了L1范数，后者是利用了L2范数。  
在这里我们先简单的介绍一下L1范数。设存在一个向量 v=[1, -1, 2]，那么该向量的L1范数即为其各元素的绝对值之和，即：  
$$ ||v||_1 = |1|+|-1|+|2| = 4 $$  
该向量的L2范数即为各元素的平方和开根，即：  
$$ ||v||_2 = \sqrt{(1)^2+(-1)^2+(2)^2}= \sqrt{6}$$

L1正则化
---

使用L1范数正则化后的目标函数如下：

$$ \min_w \sum_{i=1}^m(y_i-w^Tx_i)^2 + \lambda||w||_1 $$

其中
$$\lambda \gt 0$$
被称为正则化参数，而上式被称为LASSO（Least Absolute Shrinkage and Selection Operator）回归。

L2正则化
---

使用L2范数正则化后的目标函数如下：

$$ \min_w \sum_{i=1}^m(y_i-w^Tx_i)^2 + \lambda{||w||_2}^2 $$

同样的其正则化参数
$$ \lambda >0 $$
。上式被称为“岭回归”（ridge regression）

L1正则化与L2正则化比较
---

L1范数和L2范数正则化都有助于降低过拟合风险，让我们通过图像来直观感受一下正则化所带来的改变。假设有两个特征值，同颜色曲线上的点的损失相同。

原函数曲线等值线：  
![目标函数等高线]({{page.resource_path}}/origin.jpg)

分别将L1正则化项和L2正则化项的等值线加入上图后的函数图像：  
![加入L1和L2正则的等高线]({{page.resource_path}}/add_L1_L2.jpg)

在没有正则化项时，对于这种损失函数为凸函数的模型来说，我们要求的最佳的参数值位于最里层的也就是紫色等值线上。  
而引入正则化项后，我们不仅要求这个五颜六色的圈要小，还要使正则化项所代表的图像越小越好。注意，平方误差项与正则化项的参数应该是相同的，即我们要选的是二者等值线相交的点。由于二者都是圈越向中心缩小越好，这样一来参数就被正则化项从平方误差等值线的中心拉向坐标轴原点，减小了w1和w2的取值。使得假设函数变得更加平滑而不是猛烈的忽上忽下的剧烈变化，降低了过拟合的风险。  
至于正则化参数，我们以L1正则化项为例 假设某条等值线的值为c，即：
$$\lambda ||w||_1=c$$
，当我们将正则化参数增大一倍时，在c不变的情况下，
$$||w||_1$$
会缩小一倍，可以想象平方误差等值线就会被进一步拉向远点。故增大正则化系数会对目标函数产生更大的影响，更不易发生过拟合。  
此外，我们发现： **采用L1范数时交点常出现在坐标轴上，即w1或w2为0；采用L2范数时焦点常出现在某个象限中，即w1和w2均非0。**也就说L1正则化是更容易得到稀疏解，L2正则化得到的解更加平滑。  
由于w取得稀疏解意味着初始的所有特征中，只有w值不为0的特征值才是我们需要的，才会出现在最终的模型中，这意味着我们可以通过L1正则化来去掉不那么需要的特征值，降低模型的复杂度，其更符合奥卡姆剃刀理论。

L1正则化+L2正则化
---

当我们同时使用L1范数和L2范数时，就变成了ElasticNet回归，我们可以通过调节
$$\lambda_1$$
和
$$\lambda_2$$
来在稀疏和光滑之间进行平衡：

$$ \min_w \sum_{i=1}^m(y_i-w^Tx_i)^2 + \lambda_1 ||w||_1 + \lambda_2{||w||_2}^2 $$

---

参考资料：

* [1]周志华.机器学习[M].北京:清华大学出版社,2016：252-257.
* [2][bingo酱.L1正则化与L2正则化[OL].知乎](https://zhuanlan.zhihu.com/p/35356992)
